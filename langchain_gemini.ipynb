{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ce610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m venv venv\n",
    "# source venv/Scripts/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb6e07",
   "metadata": {},
   "source": [
    "CONFIGURACION BASICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c85aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "python-jose 3.4.0 requires pyasn1<0.5.0,>=0.4.1, but you have pyasn1 0.6.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install langchain google-generativeai python-dotenv langchain_community langchain-google-genai faiss-cpu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67109732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USUARIO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os #trabajar arhivoc de texto\n",
    "from dotenv import load_dotenv #\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load .env\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#Configurar el modelo Gemini\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    google_api_key=os.getenv(\"GENAI_API_KEY\"),\n",
    "    temperature=0.7 #Creatividad del modelo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5262c4f",
   "metadata": {},
   "source": [
    "CONVERSACION SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335d5750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo Gemini 2.0 Flash experimental (gemini-2.0-flash-exp) ofrece varios beneficios importantes, enfocados principalmente en **velocidad, eficiencia y costo-efectividad**, mientras mantiene una calidad de salida razonable. Aquí te resumo los principales beneficios:\n",
      "\n",
      "*   **Velocidad:** Es significativamente más rápido que otros modelos más grandes de Gemini. Esto lo hace ideal para aplicaciones donde la latencia es crítica, como chatbots en tiempo real, asistencia virtual interactiva y procesamiento de datos rápido.\n",
      "\n",
      "*   **Eficiencia de Costo:** Al ser un modelo más pequeño y optimizado, requiere menos recursos computacionales para ejecutarse. Esto se traduce en costos más bajos para inferencia (generar respuestas), especialmente en aplicaciones con alto volumen de solicitudes.\n",
      "\n",
      "*   **Latencia Reducida:** La velocidad de respuesta es un factor clave en la experiencia del usuario. Gemini 2.0 Flash experimental minimiza la latencia, proporcionando respuestas rápidas y fluidas, lo que mejora la satisfacción del usuario.\n",
      "\n",
      "*   **Escalabilidad:** Su eficiencia permite escalar las aplicaciones más fácilmente para manejar un mayor volumen de tráfico sin un aumento proporcional en los costos de infraestructura.\n",
      "\n",
      "*   **Adecuado para tareas específicas:** Aunque puede no ser tan preciso o creativo como los modelos más grandes para tareas complejas, Gemini 2.0 Flash experimental es excelente para tareas bien definidas y repetitivas, como:\n",
      "    *   Extracción de información\n",
      "    *   Clasificación de texto\n",
      "    *   Traducción básica\n",
      "    *   Generación de respuestas concisas y directas\n",
      "\n",
      "*   **Desarrollo iterativo:** Al ser un modelo experimental, puede ser ideal para proyectos de desarrollo iterativo, donde se necesitan prototipos rápidos y pruebas frecuentes.\n",
      "\n",
      "**En resumen, Gemini 2.0 Flash experimental es una excelente opción cuando se necesita un modelo rápido, eficiente y rentable para tareas específicas donde la velocidad es más importante que la creatividad o el razonamiento complejo.**  Es ideal para aplicaciones donde se busca optimizar costos y latencia sin sacrificar completamente la calidad de la respuesta.\n",
      "\n",
      "**Consideraciones importantes:**\n",
      "\n",
      "*   **Calidad de salida:**  Es importante tener en cuenta que la calidad de la salida puede no ser tan alta como la de modelos más grandes y complejos.  Es crucial evaluar si la calidad es suficiente para la aplicación específica.\n",
      "*   **Limitaciones:**  Como modelo experimental, puede tener limitaciones y comportamientos inesperados.  Es necesario realizar pruebas exhaustivas para asegurar su idoneidad.\n",
      "*   **Disponibilidad:** Al ser un modelo experimental, su disponibilidad puede ser limitada o estar sujeta a cambios.\n",
      "\n",
      "Antes de elegir Gemini 2.0 Flash experimental, es fundamental evaluar cuidadosamente las necesidades específicas de tu aplicación y comparar su rendimiento con otros modelos disponibles para determinar la mejor opción.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo basico de chat\n",
    "\n",
    "response = llm.invoke(\"¿Cuales son los principales beneficios de usar el modelo gemini-2.0-flash-exp?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69f247b",
   "metadata": {},
   "source": [
    "USO DE PLANTILLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13262ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear una plantilla de promt\n",
    "\n",
    "template = \"\"\"Actúa como un en experto en {tema}\n",
    "Proporciona una explicación corta y breve sobre: {concepto}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "833cbbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"tema\",\"concepto\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2646bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear una cadena\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ec5718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Claro! Aquí tienes una explicación concisa sobre las Redes Neuronales Convolucionales (CNNs):\n",
      "\n",
      "**En pocas palabras:**\n",
      "\n",
      "Las CNNs son un tipo especializado de red neuronal, **excelentes para procesar datos con estructura espacial, como imágenes y videos.**  Utilizan **capas convolucionales** que aprenden automáticamente características importantes (bordes, texturas, patrones) a través de filtros.  Estas características se combinan para reconocer objetos y patrones complejos.  También usan **capas de pooling** para reducir la dimensionalidad y hacer el modelo más robusto a variaciones en la imagen.\n",
      "\n",
      "**¿Por qué son importantes?**\n",
      "\n",
      "*   **Procesamiento de Imágenes:** Reconocimiento de objetos, clasificación de imágenes, detección facial, etc.\n",
      "*   **Procesamiento de Video:** Análisis de movimiento, detección de eventos.\n",
      "*   **Procesamiento de Lenguaje Natural:** Análisis de texto, traducción automática (en algunos casos).\n",
      "\n",
      "En resumen, son una herramienta poderosa para extraer información significativa de datos visuales y secuenciales.\n"
     ]
    }
   ],
   "source": [
    "#Ejecutar la cadena\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"tema\": \"inteligencia artificial\",\n",
    "    \"concepto\" : \"redes neuronales convolucionales\"\n",
    "})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f605b",
   "metadata": {},
   "source": [
    "CHAT CON MEMORIA \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66b7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usar memoria para hacer un chat con memoria con el modelo\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e4e192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_5552\\4133362185.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory= ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "#Configurar memoria\n",
    "\n",
    "memory= ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e08727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear plantilla para el chat\n",
    "\n",
    "template= \"\"\" La siguiente es una conversación amigable entre un humano y un asistente AI\n",
    "Historial de la conversacion: {chat_history}\n",
    "Humano: {input}\n",
    "Asistente:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74932ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"input\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "962350a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_5552\\2358553910.py:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  convesation = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "#Crear la cadena con meoria y plantilla\n",
    "\n",
    "convesation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03154815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m La siguiente es una conversación amigable entre un humano y un asistente AI\n",
      "Historial de la conversacion: []\n",
      "Humano: ¿Qué país ha ganado más mundiales de fútbol?\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Brasil ha ganado más Copas Mundiales de la FIFA, con un total de 5 títulos.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m La siguiente es una conversación amigable entre un humano y un asistente AI\n",
      "Historial de la conversacion: [HumanMessage(content='¿Qué país ha ganado más mundiales de fútbol?', additional_kwargs={}, response_metadata={}), AIMessage(content='Brasil ha ganado más Copas Mundiales de la FIFA, con un total de 5 títulos.', additional_kwargs={}, response_metadata={})]\n",
      "Humano: Nombre 10 jugadores históricos de esa selección\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Por supuesto! Aquí te dejo 10 jugadores históricos de la selección brasileña, considerando diferentes épocas y estilos:\n",
      "\n",
      "1.  **Pelé:** Considerado por muchos como el mejor futbolista de todos los tiempos. Ganó 3 Copas del Mundo (1958, 1962, 1970).\n",
      "2.  **Garrincha:** Un extremo excepcional con un regate mágico. Fue clave en los Mundiales de 1958 y 1962.\n",
      "3.  **Zico:** Un mediocampista ofensivo elegante y goleador, ídolo del Flamengo y referente de la selección en los 80.\n",
      "4.  **Sócrates:** Un mediocampista inteligente, con visión de juego y una fuerte conciencia social. Líder de la \"Democracia Corinthiana\".\n",
      "5.  **Ronaldo:** \"El Fenómeno\". Un delantero potente y letal, campeón del mundo en 1994 y 2002.\n",
      "6.  **Romário:** Un delantero oportunista y un definidor implacable. Fue fundamental para ganar el Mundial de 1994.\n",
      "7.  **Ronaldinho Gaúcho:** Un jugador creativo, con una sonrisa contagiosa y un talento inigualable. Campeón del mundo en 2002.\n",
      "8.  **Cafú:** Un lateral derecho incansable y el jugador con más partidos en la selección brasileña. Campeón del mundo en 1994 y 2002.\n",
      "9.  **Roberto Carlos:** Un lateral izquierdo con una potencia descomunal en su disparo y una gran capacidad de proyección. Campeón del mundo en 2002.\n",
      "10. **Neymar Jr.:** Un delantero habilidoso y goleador, actualmente el máximo goleador de la selección brasileña.\n",
      "\n",
      "Es una lista muy difícil de acotar, ¡hay tantos grandes jugadores en la historia de Brasil! ¿Te gustaría que te hablara de alguno de ellos en particular?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m La siguiente es una conversación amigable entre un humano y un asistente AI\n",
      "Historial de la conversacion: [HumanMessage(content='¿Qué país ha ganado más mundiales de fútbol?', additional_kwargs={}, response_metadata={}), AIMessage(content='Brasil ha ganado más Copas Mundiales de la FIFA, con un total de 5 títulos.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Nombre 10 jugadores históricos de esa selección', additional_kwargs={}, response_metadata={}), AIMessage(content='Por supuesto! Aquí te dejo 10 jugadores históricos de la selección brasileña, considerando diferentes épocas y estilos:\\n\\n1.  **Pelé:** Considerado por muchos como el mejor futbolista de todos los tiempos. Ganó 3 Copas del Mundo (1958, 1962, 1970).\\n2.  **Garrincha:** Un extremo excepcional con un regate mágico. Fue clave en los Mundiales de 1958 y 1962.\\n3.  **Zico:** Un mediocampista ofensivo elegante y goleador, ídolo del Flamengo y referente de la selección en los 80.\\n4.  **Sócrates:** Un mediocampista inteligente, con visión de juego y una fuerte conciencia social. Líder de la \"Democracia Corinthiana\".\\n5.  **Ronaldo:** \"El Fenómeno\". Un delantero potente y letal, campeón del mundo en 1994 y 2002.\\n6.  **Romário:** Un delantero oportunista y un definidor implacable. Fue fundamental para ganar el Mundial de 1994.\\n7.  **Ronaldinho Gaúcho:** Un jugador creativo, con una sonrisa contagiosa y un talento inigualable. Campeón del mundo en 2002.\\n8.  **Cafú:** Un lateral derecho incansable y el jugador con más partidos en la selección brasileña. Campeón del mundo en 1994 y 2002.\\n9.  **Roberto Carlos:** Un lateral izquierdo con una potencia descomunal en su disparo y una gran capacidad de proyección. Campeón del mundo en 2002.\\n10. **Neymar Jr.:** Un delantero habilidoso y goleador, actualmente el máximo goleador de la selección brasileña.\\n\\nEs una lista muy difícil de acotar, ¡hay tantos grandes jugadores en la historia de Brasil! ¿Te gustaría que te hablara de alguno de ellos en particular?', additional_kwargs={}, response_metadata={})]\n",
      "Humano: ¿Qué país ha ganado más mundiales de fútbol?\n",
      "Asistente:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Brasil ha ganado más Copas Mundiales de la FIFA, con un total de 5 títulos.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de conversación\n",
    "\n",
    "print(convesation.predict(input=\"¿Qué país ha ganado más mundiales de fútbol?\"))\n",
    "print(convesation.predict(input=\"Nombre 10 jugadores históricos de esa selección\"))\n",
    "print(convesation.predict(input=\"¿Qué país ha ganado más mundiales de fútbol?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2234d4f",
   "metadata": {},
   "source": [
    "ANÁLISIS DE SENTIMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a15fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "#Crear plantilla para análisis de sentimientos\n",
    "\n",
    "sentiment_template = \"\"\"Analiza el sentimiento del sigueinte texto yh clasificalo como positivo,\n",
    "negativo o neutral. Proporciona tambien una explicación de ti análisis.\n",
    "\n",
    "Texto: {texto}\n",
    "\n",
    "Formato de respuesta:\n",
    "Sentimiento: [clasificación]\n",
    "Explicación: [tu análisis]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2bee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_prompt = PromptTemplate(\n",
    "    input_variables=[\"texto\"],\n",
    "    template=sentiment_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27ec104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_chain = LLMChain(llm=llm, prompt=sentiment_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo de análisis\n",
    "\n",
    "texto_ejemplo = \"El nuevo restaurante superó todas mis expectativas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "146b74ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentimiento: Positivo\n",
      "Explicación: La frase \"superó todas mis expectativas\" indica una experiencia muy positiva. Superar las expectativas implica que el restaurante no solo cumplió con lo esperado, sino que ofreció algo aún mejor, generando una sensación de agrado y satisfacción en el hablante.\n"
     ]
    }
   ],
   "source": [
    "#Usar el método recomendado 'predict'ArithmeticError\n",
    "\n",
    "resultado = sentiment_chain.predict(texto=texto_ejemplo)\n",
    "\n",
    "#Imprimir resultado\n",
    "\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c185d2b",
   "metadata": {},
   "source": [
    "INTEGRACIONES AVANZADAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8099492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader # Cargar archivo de texto\n",
    "from langchain.text_splitter import CharacterTextSplitter # Dividir textos en textos mas chicos\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings # Trnasformar en vectores los textos \n",
    "from langchain.vectorstores import FAISS #Buscar en forma de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un archivo txt\n",
    "\n",
    "with open('ejemplo.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"\"\"\n",
    "            Los siguientes son los gastos mensuales que se tiene:\n",
    "\n",
    "            gastos Arriendo: 1200000\n",
    "\n",
    "            gastos Servicios publico: 116000\n",
    "\n",
    "            gastos Movistar casa: 29000\n",
    "\n",
    "            gastos Movistar celular: 36000\n",
    "\n",
    "            gastos Alimento: 200000\n",
    "\n",
    "            Gastos hormigas: 100000\n",
    "\n",
    "            gastos Gimnasio: 110000\n",
    "            \n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ec4bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='\n",
      "            Los siguientes son los gastos mensuales que se tiene:\n",
      "\n",
      "            gastos Arriendo: 1200000\n",
      "\n",
      "            gastos Servicios publico: 116000\n",
      "\n",
      "            gastos Movistar casa: 29000\n",
      "\n",
      "            gastos Movistar celular: 36000\n",
      "\n",
      "            gastos Alimento: 200000\n",
      "\n",
      "            Gastos hormigas: 100000\n",
      "\n",
      "            gastos Gimnasio: 110000\n",
      "\n",
      "            ' metadata={'source': 'ejemplo.txt'}\n"
     ]
    }
   ],
   "source": [
    "#Cargar y procesar el documento\n",
    "\n",
    "loader = TextLoader('ejemplo.txt', encoding='utf-8')\n",
    "documents = loader.load()\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c08b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 66, which is longer than the specified 10\n",
      "Created a chunk of size 36, which is longer than the specified 10\n",
      "Created a chunk of size 44, which is longer than the specified 10\n",
      "Created a chunk of size 39, which is longer than the specified 10\n",
      "Created a chunk of size 42, which is longer than the specified 10\n",
      "Created a chunk of size 35, which is longer than the specified 10\n",
      "Created a chunk of size 35, which is longer than the specified 10\n",
      "Created a chunk of size 35, which is longer than the specified 10\n"
     ]
    }
   ],
   "source": [
    "#Dividir el texto en chunks\n",
    "\n",
    "text_splitter= CharacterTextSplitter(chunk_size=10, chunk_overlap=1)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cb1662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gastos Servicios publico: 116000'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c818dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear embeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=os.getenv(\"GENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b5592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear base de datos vectorial\n",
    "\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7748a0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gastos Arriendo: 1200000\n"
     ]
    }
   ],
   "source": [
    "#Realizar una busuqeda\n",
    "\n",
    "query = \"gastos apartamento\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7e4c9",
   "metadata": {},
   "source": [
    "CREACION DE UN AGENTE PERSONALZIADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945d389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener la hora actual\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
